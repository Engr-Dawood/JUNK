{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Trannsformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EatcBhZfP4Ml"
   },
   "source": [
    "The provided lab description covers the implementation of several key components of the Transformer architecture. However, it does not encompass the entire architecture as described in the original \"Attention is All You Need\" paper by Vaswani et al. Specifically, it focuses on implementing components such as self-attention mechanism, feed-forward network, positional encoding, and a single transformer block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V49MJ-QPb5c"
   },
   "source": [
    "## Task 1-Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34eZpenC23hC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention module.\n",
    "\n",
    "        Args:\n",
    "        - embed_size (int): The dimensionality of input embeddings.\n",
    "        - heads (int): The number of attention heads.\n",
    "\n",
    "        This module computes the self-attention mechanism. It takes input embeddings and splits them into multiple heads.\n",
    "        Then, it computes attention scores between query, key, and value vectors and aggregates the values based on these scores.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "        - values (torch.Tensor): The values tensor.\n",
    "        - keys (torch.Tensor): The keys tensor.\n",
    "        - query (torch.Tensor): The query tensor.\n",
    "        - mask (torch.Tensor): The mask tensor.\n",
    "\n",
    "        Returns:\n",
    "        - out (torch.Tensor): The output tensor.\n",
    "\n",
    "        This function performs the forward pass of the self-attention mechanism. It computes attention scores between\n",
    "        query and key vectors, applies the mask if provided, computes attention weights using softmax, and finally\n",
    "        aggregates the values based on these weights.\n",
    "\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qZhbFLvPe78"
   },
   "source": [
    "## Task 2-Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4x712h9Lpvb"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward module.\n",
    "\n",
    "        Args:\n",
    "        - embed_size (int): The dimensionality of input embeddings.\n",
    "        - ff_hidden_size (int): The hidden layer size of the feed-forward network.\n",
    "\n",
    "        This module implements a simple feed-forward network with one hidden layer and ReLU activation function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the feed-forward network.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - x (torch.Tensor): The output tensor.\n",
    "\n",
    "        This function performs the forward pass of the feed-forward network. It applies the linear transformation\n",
    "        followed by ReLU activation and another linear transformation.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Acl7Uwj4PjuW"
   },
   "source": [
    "## Task 3-Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5id4Kkf6LrnE"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "        - embed_size (int): The dimensionality of input embeddings.\n",
    "        - max_len (int): The maximum length of input sequences.\n",
    "\n",
    "        This module generates positional encodings for input sequences based on their positions.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the positional encoding module.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - x (torch.Tensor): The output tensor.\n",
    "\n",
    "        This function performs the forward pass of the positional encoding module. It adds positional encodings\n",
    "        to the input embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juxSSZPMPTqO"
   },
   "source": [
    "## Task 4-Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-Q8iF4cpLTwE"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden_size, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the TransformerBlock module.\n",
    "\n",
    "        Args:\n",
    "        - embed_size (int): The dimensionality of input embeddings.\n",
    "        - heads (int): The number of attention heads.\n",
    "        - ff_hidden_size (int): The hidden layer size of the feed-forward network.\n",
    "        - dropout (float): The dropout probability.\n",
    "\n",
    "        This module implements a single transformer block consisting of multi-head self-attention mechanism,\n",
    "        feed-forward network, and layer normalization.\n",
    "\n",
    "        \"\"\"\n",
    "        #sequence of architectural components should be self_attention, feed_forward, layer_norm, dropout layers respectuvely.\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer block.\n",
    "\n",
    "        Args:\n",
    "        - value (torch.Tensor): The value tensor.\n",
    "        - key (torch.Tensor): The key tensor.\n",
    "        - query (torch.Tensor): The query tensor.\n",
    "        - mask (torch.Tensor): The mask tensor.\n",
    "\n",
    "        Returns:\n",
    "        - out (torch.Tensor): The output tensor.\n",
    "\n",
    "        This function performs the forward pass of the transformer block. It applies the self-attention mechanism,\n",
    "        feed-forward network, and layer normalization.\n",
    "\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NpRFSMNJLQrE"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        ff_hidden_size,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        max_len,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, embed_size)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, ff_hidden_size, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        N, source_length = source.shape\n",
    "        N, target_length = target.shape\n",
    "\n",
    "        source_embedding = self.dropout(self.positional_encoding(self.encoder_embedding(source)))\n",
    "        target_embedding = self.dropout(self.positional_encoding(self.decoder_embedding(target)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            target_embedding = layer(source_embedding, source_embedding, target_embedding, source_mask)\n",
    "\n",
    "        output = self.fc_out(target_embedding)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjuMYeMwPNEs",
    "outputId": "3fae8b6f-a990-4807-eaa2-95835a3f1c85"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "ff_hidden_size = 512\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "input_vocab_size = 1000  # Example vocab size\n",
    "target_vocab_size = 1000  # Example vocab size\n",
    "max_len = 100\n",
    "\n",
    "# Create random input data for testing\n",
    "source = torch.randint(0, input_vocab_size, (32, 10))  # Batch size 32, sequence length 10\n",
    "target = torch.randint(0, target_vocab_size, (32, 5))  # Batch size 32, sequence length 5\n",
    "\n",
    "# Create masks\n",
    "source_mask = torch.ones((32, 1, 1, 10))  # For padding mask\n",
    "target_mask = torch.ones((32, 1, 5, 5))  # For padding mask\n",
    "\n",
    "# Initialize transformer model\n",
    "model = Transformer(\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    ff_hidden_size,\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    max_len,\n",
    "    dropout,\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = model(source, target, source_mask, target_mask)\n",
    "\n",
    "# Print output shape\n",
    "print(\"Output shape:\", output.shape)  # It should be (batch_size, target_sequence_length, target_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5-Flow Chart \n",
    "Understand the flow of the code, and draw the block digram/flowchart to explain the flow. You can handdraw the digram and include the picture in your notebook of the lab report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6-Changing input data\n",
    "Also, try to modify the test data a little bit and study the impact of transformer's architecture on that changed data and explain the change in your words that you witness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "\n",
    "Please strcitly adhere to the submission guidleines, i.e.,  submit the .ipynb file with complete code and diagram of flow chart embedded in the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
