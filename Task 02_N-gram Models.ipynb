{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 02-N-gram Models</h1>\n",
    "<p>N-gram models are a type of probabilistic language model used in natural language processing and speech recognition. They predict the likelihood of a word given the previous words in a sequence. In this Task, we will implement a simple N-gram model and apply it to a text dataset. <p\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Preparation:\n",
    "<ul>\n",
    "    <li>Load the text dataset provided. Dataset Link: </li>\n",
    "    <li>Preprocess the text by converting it to lowercase and removing punctuation.</li>\n",
    "    <li>Split the text into tokens (words).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by converting it to lowercase and removing punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates tokens (words) from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of tokens (words).\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Building N-gram Model:\n",
    "<ul>\n",
    "  <li>  Implement a function to generate N-grams of a given text and N.\n",
    "    <li>Calculate the frequencies of each N-gram in the dataset.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the generate_ngrams function, start by understanding its function signature, which takes in a list of tokens (tokens) and an integer N, representing the desired length of the N-grams, and returns a list of N-grams. Begin by creating an empty list to store the generated N-grams. Then, iterate through the list of tokens using a loop. For each token, create an N-gram by combining it with the next N-1 tokens (if available), ensuring not to go out of bounds. Append each generated N-gram to the list. Finally, handle edge cases, such as when the number of tokens is less than N, and test your implementation with various inputs to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens: List[str], N: int) -> List[Tuple[str]]:\n",
    "    \"\"\"\n",
    "    Generates N-grams from the list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of tokens (words).\n",
    "        N (int): The value of N for N-grams.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str]]: List of N-grams.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the build_ngram_model function, begin by understanding its function signature. This function takes in a string of text (text) and an integer N, representing the desired length of the N-grams, and returns the N-gram model represented as a dictionary where keys are N-grams (tuples of N words) and values are the frequencies of occurrence of each N-gram in the text. Start by preprocessing the input text, converting it to lowercase and removing punctuation. Then, tokenize the preprocessed text into individual words or tokens. Next, generate N-grams from the tokens using the generate_ngrams function implemented earlier. Iterate through the generated N-grams and count the frequency of each N-gram. Store these frequencies in a dictionary where the keys are the N-grams and the values are their frequencies. Finally, return the N-gram model. Test your implementation with different texts and N values to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(text: str, N: int) -> Dict[Tuple[str], int]:\n",
    "    \"\"\"\n",
    "    Builds the N-gram model from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        N (int): The value of N for N-grams.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str], int]: The N-gram model represented as a dictionary where keys are N-grams and values are frequencies.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Predictive Model <br>\n",
    "<li>Implement a function to predict the next word given the previous N-1 words.\n",
    "<li>Test the predictive model on sample input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To implement the predict_next_word function, start by understanding its function signature. This function takes in the N-gram model (model) represented as a dictionary and a tuple of previous words (prev_words). It aims to predict the next word based on the previous words and the frequencies of N-grams in the model. First, check if there are any N-grams in the model that match the previous words. If there are matching N-grams, retrieve all possible next words associated with those N-grams and randomly select one of them as the predicted next word. If no next words are found, return None. Ensure to handle edge cases, such as when the previous words are not found in the model. Test your implementation with different N-gram models and input word sequences to validate its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model: Dict[Tuple[str], int], prev_words: Tuple[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Predicts the next word given the previous words and the N-gram model.\n",
    "\n",
    "    Args:\n",
    "        model (Dict[Tuple[str], int]): The N-gram model.\n",
    "        prev_words (Tuple[str]): Tuple of previous words.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The predicted next word, or None if no next word is found.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Evaluating n-gram models <br>\n",
    "<h3>What is Perplexity</h3>\n",
    "Perplexity is a measurement used to evaluate the performance of a language model, such as an N-gram model, in predicting a sequence of words. It quantifies how well the model predicts unseen data or text. A lower perplexity indicates that the model assigns higher probabilities to unseen words, suggesting better generalization ability.<br>\n",
    "\n",
    "### ðŸ”¢ Steps to Implement Perplexity Calculation\n",
    "\n",
    "To compute the **perplexity** of a test dataset using a trained N-gram model, follow these steps:\n",
    "\n",
    "1. **Preprocess the test text**  \n",
    "   Apply the same preprocessing steps (e.g., lowercasing, punctuation removal) that were used during training.\n",
    "\n",
    "2. **Tokenize the test text**  \n",
    "   Split the preprocessed text into individual word tokens.\n",
    "\n",
    "3. **Generate N-grams**  \n",
    "   Use the same N value (e.g., bigram, trigram) as in the trained model to create N-grams from the tokenized test text.\n",
    "\n",
    "4. **Calculate log probabilities**  \n",
    "   For each N-gram in the test set, retrieve its log probability from the trained N-gram model.\n",
    "\n",
    "5. **Compute the average log probability**  \n",
    "   Sum all the log probabilities and divide by the number of N-grams to get the average.\n",
    "\n",
    "6. **Calculate perplexity**  \n",
    "\n",
    "   Take the exponential of the negative average log probability to obtain the perplexity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(model: Dict[Tuple[str], int], test_text: str, N: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the perplexity of the N-gram model on the test text.\n",
    "\n",
    "    Args:\n",
    "        model (Dict[Tuple[str], int]): The N-gram model.\n",
    "        test_text (str): The test text.\n",
    "        N (int): The value of N for N-grams.\n",
    "\n",
    "    Returns:\n",
    "        float: The perplexity value.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    #perplexity = 2 ** (-total_log_probability / len(test_ngrams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Cases</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for preprocess_text function\n",
    "preprocess_text_test_cases = [\n",
    "    (\"This is a Test.\", \"this is a test\"),\n",
    "    (\"Hello World!\", \"hello world\"),\n",
    "    (\"A B C D E F G\", \"a b c d e f g\")\n",
    "]\n",
    "\n",
    "# Test cases for generate_tokens function\n",
    "generate_tokens_test_cases = [\n",
    "    (\"This is a test.\", [\"This\", \"is\", \"a\", \"test.\"]),\n",
    "    (\"Another example.\", [\"Another\", \"example.\"]),\n",
    "    (\"123 456 789\", [\"123\", \"456\", \"789\"])\n",
    "]\n",
    "\n",
    "# Test cases for generate_ngrams function\n",
    "generate_ngrams_test_cases = [\n",
    "    ([\"This\", \"is\", \"a\", \"test\"], 2, [(\"This\", \"is\"), (\"is\", \"a\"), (\"a\", \"test\")]),\n",
    "    ([\"Another\", \"example\"], 2, [(\"Another\", \"example\")]),\n",
    "    ([\"123\", \"456\", \"789\"], 2, [(\"123\", \"456\"), (\"456\", \"789\")])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Run test cases\n",
    "print(\"\\nRunning Test Cases:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Test preprocess_text function\n",
    "print(\"\\nTesting preprocess_text function:\")\n",
    "for test_case in preprocess_text_test_cases:\n",
    "    input_text, expected_output = test_case\n",
    "    print(expected_output)\n",
    "    print(preprocess_text(input_text))\n",
    "    assert preprocess_text(input_text) == expected_output\n",
    "print(\"All preprocess_text test cases passed.\")\n",
    "\n",
    "# Test generate_tokens function\n",
    "print(\"\\nTesting generate_tokens function:\")\n",
    "for test_case in generate_tokens_test_cases:\n",
    "    input_text, expected_output = test_case\n",
    "    assert generate_tokens(input_text) == expected_output\n",
    "print(\"All generate_tokens test cases passed.\")\n",
    "\n",
    "# Test generate_ngrams function\n",
    "print(\"\\nTesting generate_ngrams function:\")\n",
    "for test_case in generate_ngrams_test_cases:\n",
    "    input_tokens, n, expected_output = test_case\n",
    "    assert generate_ngrams(input_tokens, n) == expected_output\n",
    "print(\"All generate_ngrams test cases passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this task, you are required to evaluate the model on the dataset given (TextFile.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read text from file\n",
    "    with open(\"TextFile.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    print(\"Preprocessed Text:\", preprocessed_text)\n",
    "\n",
    "    # Generate tokens\n",
    "    tokens = generate_tokens(preprocessed_text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Generate N-grams\n",
    "    N = 3\n",
    "    ngrams = generate_ngrams(tokens, N)\n",
    "    print(f\"{N}-grams:\", ngrams)\n",
    "\n",
    "    # Build N-gram model\n",
    "    ngram_model = build_ngram_model(tokens, N)\n",
    "    print(f\"{N}-gram Model:\", ngram_model)\n",
    "\n",
    "    # Predict next word\n",
    "    prev_words = ('quick', 'brown', )\n",
    "    next_word = predict_next_word(ngram_model, prev_words)\n",
    "    print(\"Predicted Next Word:\", next_word)\n",
    "\n",
    "    # Evaluate perplexity\n",
    "    test_text = \"This is a test.\"\n",
    "    perplexity = evaluate_perplexity(ngram_model, test_text, N)\n",
    "    print(\"Perplexity:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Deliverables :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit .ipynb notebook with your name and registration number clearly written on the top. No need to submit the text data alongwith. Please strcitly follow the submission guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
